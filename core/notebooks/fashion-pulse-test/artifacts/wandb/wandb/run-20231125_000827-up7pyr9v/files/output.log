
Epoch 0:  11%|███████▎                                                        | 103/907 [00:01<00:11, 67.28it/s, loss=0.429, v_num=yr9v]
/home/loewe/.local/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
[34m[1mwandb[39m[22m: logging graph, to disable use `wandb.watch(log_graph=False)`
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/loewe/.local/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /home/loewe/tabs/pro-dl/notebooks/fashion-pulse-test/artifacts/best_models exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
  | Name      | Type               | Params
-------------------------------------------------
0 | model     | Model              | 101 K
1 | train_acc | MulticlassAccuracy | 0
2 | valid_acc | MulticlassAccuracy | 0
3 | test_acc  | MulticlassAccuracy | 0
-------------------------------------------------
101 K     Trainable params
0         Non-trainable params
101 K     Total params
0.407     Total estimated model params size (MB)
/home/loewe/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/loewe/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.








Epoch 1:  95%|███████████████████████▋ | 858/907 [00:10<00:00, 84.40it/s, loss=0.144, v_num=yr9v, val_loss=0.187, valid_acc_epoch=0.948]



Epoch 2:  86%|██████████████████████▍   | 782/907 [00:09<00:01, 84.35it/s, loss=0.11, v_num=yr9v, val_loss=0.145, valid_acc_epoch=0.958]




Epoch 3:  91%|█████████████████████▊  | 824/907 [00:10<00:01, 78.83it/s, loss=0.0807, v_num=yr9v, val_loss=0.131, valid_acc_epoch=0.962]



Epoch 4:  83%|████████████████████▊    | 756/907 [00:08<00:01, 85.57it/s, loss=0.063, v_num=yr9v, val_loss=0.130, valid_acc_epoch=0.961]


Validation DataLoader 0:  92%|██████████████████████████████████████████████████████████████████      | 144/157 [00:01<00:00, 87.17it/s]
`Trainer.fit` stopped: `max_epochs=5` reached.
/home/loewe/.local/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
Epoch 4: 100%|█████████████████████████| 907/907 [00:10<00:00, 83.71it/s, loss=0.063, v_num=yr9v, val_loss=0.124, valid_acc_epoch=0.965]
Testing DataLoader 0: 100%|██████████████████████████████████████████████████████████████████████████| 313/313 [00:02<00:00, 127.34it/s]
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
       Test metric             DataLoader 0
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
     test_acc_epoch         0.9668999910354614
        test_loss           0.11184007674455643
────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────